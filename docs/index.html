<?xml version="1.0"
      encoding="UTF-8"
?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title> RODSNet: End-to-end Real-time Obstacle Detection Network for Safe Self-driving via Multi-task Learning </title>
<meta name="generator" content="Nested http://nestededitor.sourceforge.net/" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
    menuSettings: {zoom: "Double-Click", zscale: "300%"},
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
    MathMenu: {showRenderer: false},
    "HTML-CSS": {
        availableFonts: ["TeX"],
        preferredFont: "TeX",
        imageFont: null
    }
  });
</script>
<style type="text/css">
    body { background-color: White; font-family: Helvetica, Futura, "Trebuchet MS", sans-serif; width:900px; margin:0 auto;}
    h1 { color: black; font-family: Helvetica, Futura, "Trebuchet MS", sans-serif; }
    p { color: black; font-family: Helvetica, Futura, "Trebuchet MS", sans-serif;}
</style>
<style>
.aligncenter {
    text-align: center;
}
</style>


</head>
<body>

<p>&nbsp;</p>
<p>&nbsp;</p>
<div id="header" class="header" align="center">
<h1> End-to-end Real-time Obstacle Detection Network for Safe Self-driving via Multi-task Learning </h1>
<p style="text-align:center">
    <font size="4"> <a href="mailto:tjsong@rit.kaist.ac.kr" target="_blank">Taek-jin Song</a> 
      <sup>*,	<span>&#8224;</span> </sup>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <font size="4"> <a href="mailto:jojeong@rit.kaist.ac.kr" target="_blank">Jongoh Jeong</a> 
      <sup>*,	<span>&#8224;</span> </sup>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <font size="4"> <a href="mailto:johkim@rit.kaist.ac.kr" target="_blank">Jong-Hwan Kim</a> 
      <sup><span>&#8224;</span></sup>
    </font>
</p>
<p style="text-align:left">
  <i><font size="4">
      <sup><span>&#8224;</span></sup>School of Electrical Engineering, Korea Advanced Institute of Science and Technology (<b><a href="https://kaist.ac.kr/">KAIST</a></b>)  
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <sup>*</sup>Equal Contribution
  </i>
</p>
</div>

<hr class="heavy" />
<h2>Abstract</h2>
<div id="body" class="body">
<div id="section1" class="section">
<p>
Semantic segmentation and depth estimation lie at  the heart of scene understanding and play crucial roles especially for autonomous driving. In particular, it is desirable for an intelligent self-driving agent to discern unexpected obstacles on the road ahead reliably in real-time. While existing semantic segmentation studies for small road hazard detection have incorporated fusion of multiple modalities, they require additional sensor inputs and are often limited by a heavyweight network for real-time processing. In this light, we propose an end-to-end Real-time Obstacle Detection via Simultaneous refinement, coined RODSNet, which jointly learns semantic segmentation and disparity maps from a stereo RGB pair and refines them simultaneously in a single module. RODSNet exploits two efficient single-task network architectures and a simple refinement module in a multi-task learning scheme to recognize unexpected small obstacles on the road. We validate our method by fusing Cityscapes and Lost and Found datasets and show that our method outperforms previous approaches on the obstacle detection task, even recognizing the unannotated obstacles at 14.5 FPS on our fused dataset (2048×1024 resolution) using RODSNet-2×. In addition, extensive ablation studies demonstrate that our simultaneous refinement effectively facilitates contextual learning between semantic and depth information.
</p>

<figure class="aligncenter">
  <img src="overall_network_v3.png" alt="pipeline" style="width:100%">
  <!-- <img src="overall_network_v3_basefeatext.png" alt="pipeline" style="width:100%">
  <img src="overall_network_v3_initial_est.png" alt="pipeline" style="width:100%">
  <img src="overall_network_v3_refinement.png" alt="pipeline" style="width:100%"> -->
</figure>

<p>  
  <!-- <a href="https://arxiv.org/abs/?????" target="_blank">[IEEE Xplorer Link]</a> -->
  <a href="https://drive.google.com/file/d/1ngFoE8Cpq4HxsGZho60R11zaJOBdGD0G/view?usp=sharing" target="_blank">[PDF]</a>
  <a href="https://github.com/SAMMiCA/RODSNet" target="_blank">[Code]</a>
 </p>

<hr class="heavy" /></div>
<h2>Demo</h2>
<p>We demonstrate the working demo of our network on real-world autonomous driving scenes.</p>
<table style="width:100%">
  <tr>
    <th>
	    <video width="840" height="480" controls>
		  <source src="https://rit.kaist.ac.kr/wp-content/uploads/2022/02/RODSNet_demo_upload.mp4" type="video/mp4" alt="video demonstration">
		  Your browser does not support HTML5 video.
		</video>
    </th>
  </tr>
</table>



<hr class="heavy" />
<h2>Proposed Network</h2>
<div id="body" class="body">
<div id="section1" class="section">
<p>
We propose an end-to-end network for Real-time Obstacle Detection via Simultaneous refinement, termed RODSNet. Given a pair of stereo RGB images, RODSNet jointly refines initial semantic and disparity maps in one stage, discerning both trained and unexpected roadside obstacles. Our network builds upon efficient single-task network architectures to estimate initial semantic segmentation and disparity maps, and adds a simple simultaneous refinement module to further improve both results. While we target real-time obstacle detection, this refinement module can also be coupled with other high-performing single-task networks at the expense of an increased computation time. 
<br/><br/>
We highlight our main contributions in three-fold:
<br/>
1. We propose a simple and efficient refinement module that simultaneously improves the predicted semantic segmentation and disparity maps. In contrast to other attention-based approaches, our module exploits a stacked hourglass network to promote contextual refinement with the initial semantic and disparity maps, and the left (reference) RGB image as inputs.
<br/><br/>
2. Our network outperforms existing semantic segmentation networks for road obstacle detection. RODSNet−2× achieves 2.6% and 1.7% improvements in segmentation accuracy for small obstacles (IoU) and all classes (mIoU), respectively, at 14.5 FPS inference speed on our fused dataset (2048×1024 resolution). Moreover, our ablation study demonstrates that it can be further tuned to better detect obstacles across all depth ranges than previous
methods. 
<br/><br/>
3. The proposed network can be generalized to detect unannotated obstacles on the road. RODSNet effectively detects obstacles even in the absence of proper annotations
</p>

<hr class="heavy" />
<h2>Network Modules</h2>
<div id="body" class="body">
<div id="section1" class="section">
<p>
  Our multi-task learning network consists of the following modules: Base feature extractor, initial semantic segmentation and disparity map estimation, and simultaneous refinement.

  <br><br><br>
  <i>Base feature extractor</i>
  
  <figure class="aligncenter">
    <img src="overall_network_v3_basefeatext.png" alt="Base feature extractor" style="width:50%">
  </figure>

  <i>Initial Map Estimation</i>
  
  <figure class="aligncenter">
    <img src="overall_network_v3_initial_est.png" alt="Initial map estimation" style="width:50%">
  </figure>

  <i>Simultaneous Refinement</i>

  <figure class="aligncenter">
    <img src="overall_network_v3_refinement.png" alt="Simultaneou refinement" style="width:50%">
  </figure>
</p>


<hr class="heavy" /></div>
<div id="section8" class="section">
  <h2 id="bibtex">Citation</h2>
<pre>
@article{songjeong2021rodsnet,
    author = {Song, Taek-jin and Jeong, Jongoh and Kim, Jong-Hwan},
    title = {End-to-end Real-time Obstacle Detection Network for Safe Self-driving via Multi-task Learning},
    year = {2022},
    doi = {000},
    URL = {https://doi.org/000},
    journal = {IEEE Transactions on Intelligent Transportation Systems (T-ITS)}
}
</pre>
</div>

<hr class="heavy" />
<h2> Acknowledgements </h2>
<p>This work was supported by the Institute for Information & Communications Technology Promotion (IITP) grant funded by the Korea government (MSIT) (No.2020-0-00440, Development of Artificial Intelligence Technology that Continuously Improves Itself as the Situation Changes in the Real World). </p>